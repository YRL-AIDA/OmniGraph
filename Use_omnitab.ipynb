{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2120088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.8/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (20.9)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->accelerate) (2.4.7)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.5.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/lib/python3.8/site-packages (from networkx->torch>=1.10.0->accelerate) (4.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60549f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig,AutoModelForCausalLM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c172e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c193cad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-337b3dee091a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2956\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m                 )\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[0;32m-> 1160\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1161\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"neulab/omnitab-large-finetuned-wtq\")\n",
    "# IMPORTANT: the initial BART model's decoding is penalized by no_repeat_ngram_size, and thus\n",
    "# we should disable it here to avoid problematic generation\n",
    "config.no_repeat_ngram_size = 3 # В omnitab = 3, part exec = 0\n",
    "config.max_length = 1024\n",
    "#config.early_stopping = False #  В омнитаб нет\n",
    "\n",
    "#model = AutoModel.from_config(config) # Для создания экземпляра модели без предобученных весов\n",
    "tokenizer = AutoTokenizer.from_pretrained('neulab/omnitab-large-finetuned-wtq')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq')\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"bert-base-uncased\", device_map = 'auto')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbeceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 2008']\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'year': [1896, 1900, 1904, 2004, 2008, 2012],\n",
    "    'city': ['athens', 'paris', 'st. louis', 'athens', 'beijing', 'london']\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "query = 'In which year did beijing host the Olympic Games?'\n",
    "encoding = tokenizer(table=table, query=query, return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(**encoding)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6265fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d400d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5ea624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d47faa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1fcb55ede74846b4f58c3975bbb5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a1b79a85d49b8b655dca03816dd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e4f5700a04958992b2f0915d3afae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\"train\":\"data/wtq_lf_preorder/train/data-00000-of-00001.arrow\",\n",
    "              \"test\":\"data/wtq_lf_preorder/test/data-00000-of-00001.arrow\",\n",
    "              \"validation\":\"data/wtq_lf_preorder/validation/data-00000-of-00001.arrow\"}\n",
    "datasets = load_dataset(\"arrow\", data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64842ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'answers', 'table', 'lf_p', 'lf_pc', 'lf_pcs', 'lf_pcsgbh', 'lf_pcsgbhob', 'lf_pcsgbhoba', 'lf_pcsgbhobaop', 'lf_pcsgbhobaopl', 'sql'],\n",
       "        num_rows: 8610\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'answers', 'table', 'lf_p', 'lf_pc', 'lf_pcs', 'lf_pcsgbh', 'lf_pcsgbhob', 'lf_pcsgbhoba', 'lf_pcsgbhobaop', 'lf_pcsgbhobaopl', 'sql'],\n",
       "        num_rows: 4344\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'answers', 'table', 'lf_p', 'lf_pc', 'lf_pcs', 'lf_pcsgbh', 'lf_pcsgbhob', 'lf_pcsgbhoba', 'lf_pcsgbhobaop', 'lf_pcsgbhobaopl', 'sql'],\n",
       "        num_rows: 2092\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94a65563",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = f\"lf_pcs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b45013",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"question\",\"table\"]\n",
    "def length_filter(example):\n",
    "    return {f\"length_{target_col_name}\": len(tokenizer.tokenize(example[target_col_name]))<=max_target_length}\n",
    "\n",
    "for mode in ['train',\"validation\"]:\n",
    "\n",
    "    if mode ==\"train\":\n",
    "        datasets[mode] = datasets[mode].map(length_filter)\n",
    "        filter_col = [idx for idx, i in enumerate(datasets[mode][filter_col_name]) if i==1]\n",
    "        logger.info(f\"remove {len(datasets[mode])-len(filter_col)} observations with length > {max_target_length}.\")\n",
    "        logger.info(f\"Previous {mode} size {len(datasets[mode])} actual {mode} size {len(filter_col)}\")\n",
    "        datasets[mode] = datasets[mode].select(filter_col)\n",
    "\n",
    "    removed_cols = set(datasets[mode].features.keys())-set([target_col_name]+features)\n",
    "    datasets[mode] = datasets[mode].remove_columns(removed_cols)\n",
    "    datasets[mode] = datasets[mode].rename_column(target_col_name, 'answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4510036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x7ff016c9e460> Reading dataset from /home/jovyan/cloud/postgraduate/works/Partial-Exec/data/tables/tagged/pristine-seen-tables.tagged\n",
      "<ipykernel.iostream.OutStream object at 0x7ff016c9e460> Reading dataset from /home/jovyan/cloud/postgraduate/works/Partial-Exec/data/tables/tagged/pristine-unseen-tables.tagged\n",
      "<ipykernel.iostream.OutStream object at 0x7ff016c9e460> Reading dataset from /home/jovyan/cloud/postgraduate/works/Partial-Exec/data/tables/tagged/training.tagged\n"
     ]
    }
   ],
   "source": [
    "from data_processing.metrics import (to_value_list,\n",
    "                                     to_value,\n",
    "                                     plot_and_save_model_performance,\n",
    "                                     fuzzy_matching,\n",
    "                                     strict_denotation_accuracy,\n",
    "                                     flexible_denotation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89106836",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'{path_model_folder}/model_evaluation_results.json'\n",
    "target_col_name = f\"lf_{Omega_include.lower()}\"\n",
    "Omega_include = path_model_folder.split('/')[-1].split('_')[1]\n",
    "datasets = load_from_disk(f\"data/wtq_lf_{args.flatten_mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tokenizer_config = \"microsoft/tapex-large\"\n",
    "tokenizer = TapexTokenizer.from_pretrained(path_tokenizer_config)\n",
    "config = AutoConfig.from_pretrained(path_tokenizer_config)\n",
    "config.no_repeat_ngram_size = 0\n",
    "config.max_length = 1024\n",
    "config.early_stopping = False\n",
    "\n",
    "features = [\"question\", \"table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_cols = set(datasets[\"validation\"].features.keys())-set([target_col_name]+features+[\"answers\"])\n",
    "datasets[\"validation\"] = datasets[\"validation\"].remove_columns(removed_cols)\n",
    "datasets[\"validation\"] = datasets[\"validation\"].rename_column(target_col_name, 'answers_lf')\n",
    "datasets[\"validation\"] = datasets[\"validation\"].rename_column(\"answers\", 'answers_exec')\n",
    "datasets[\"test\"] = datasets[\"test\"].rename_column(\"answers\", 'answers_exec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd894ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = random.randint(0, len(datasets[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b762b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tableqa_function(examples, is_test=False):\n",
    "\n",
    "        questions = [question.lower() for question in examples[\"question\"]]\n",
    "        example_tables = examples[\"table\"]\n",
    "        tables = [\n",
    "            pd.DataFrame.from_records(example_table[\"rows\"], columns=example_table[\"header\"])\n",
    "            for example_table in example_tables\n",
    "        ]\n",
    "        \n",
    "        model_inputs = tokenizer(\n",
    "            table=tables, query=questions, max_length=1024, padding=False, truncation=True\n",
    "        )\n",
    "        model_inputs[\"answers_exec\"] = examples[\"answers_exec\"] \n",
    "        if not is_test:\n",
    "            model_inputs[\"answers_lf\"] = examples[\"answers_lf\"]\n",
    "            \n",
    "        return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6db88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, comp_graph = False, is_test=False,flatten_mode=\"preorder\"):\n",
    "        model.eval()\n",
    "        perfs={ \"Strict_Denotation_Accuracy_Execs\":[]}\n",
    "        for idx, example in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            print(f\"idx {idx}\")\n",
    "            input_ids = example[\"input_ids\"]\n",
    "            attention_mask = example[\"attention_mask\"]\n",
    "\n",
    "            input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "            attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
    "            generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            prediction_lf = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True).strip()\n",
    "            if comp_graph:\n",
    "                header=tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)\n",
    "                header=header.split(' col : ')[1].split(' row 1 : ')[0].split(' |')\n",
    "                header=[h.strip() for h in header]\n",
    "                #G=parse(prediction_lf, header, flatten_mode=args.flatten_mode)\n",
    "                try:\n",
    "                    print('1')\n",
    "                    G=parse(prediction_lf, header, flatten_mode=flatten_mode)\n",
    "                    print('2')\n",
    "                    prediction_exec = G.executed_last_node()\n",
    "                    print(f'answers_exec {answers_exec} prediction_exec {prediction_exec}')\n",
    "                    acc_ex_stric = strict_denotation_accuracy(to_value_list(answers_exec), to_value_list(prediction_exec))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    acc_ex_stric = False\n",
    "\n",
    "            \n",
    "            else:\n",
    "                acc_ex_stric = strict_denotation_accuracy(to_value_list(answers_exec), to_value_list(prediction_exec))\n",
    "            print(f'Strict_Denotation_Accuracy_Execs {acc_ex_stric}')\n",
    "            perfs[\"Strict_Denotation_Accuracy_Execs\"].append(acc_ex_stric)\n",
    "\n",
    "        Strict_Denotation_Accuracy_Exec = sum(perfs[\"Strict_Denotation_Accuracy_Execs\"])/len(perfs[\"Strict_Denotation_Accuracy_Execs\"])\n",
    "\n",
    "        return {'Strict_Denotation_Accuracy_Exec': Strict_Denotation_Accuracy_Exec}  # example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
